import numpy as np
import densedata_pb2 as dataset

# Define state order
NUM_BODY_PARTS = 12;

BODY = 0
RTHIGH = 1
RCALF = 2
RFOOT = 3
LTHIGH = 4
LCALF = 5
LFOOT = 6
RUARM = 7
RLARM = 8
LUARM = 9
LLARM = 10
HEAD = 11

def extract_states(f):
  """Extract the images into a 3D numpy array [game, timestep, [x, y, th, dx, dy, dth]].
  Args:
    f: A file object that can be passed into a gzip reader.
  Returns:
    data: A 4D uint8 numpy array [index, y, x, depth].
  Raises:
    ValueError: If the bytestream does not start with 2051.
  """
    #f = open("../../denseData_2017-11-06_08-58-03.proto", "rb")

  dat = dataset.DataSet()
  try:
      dataset.ParseFromString(f.read())
      f.close
  except IOError:
      print "Import failed"


    num_games = len(dat.denseData)


  bodyX = np.empty(shape=(len(dat.denseData[0].state), 1), dtype=np.float32)


  for idx, s in enumerate(dataset.denseData[0].state):

      np.put(bodyX, ind=idx, v=s.body.x)
      np.put(bodyY, ind=idx, v=s.body.y)
      np.put(bodyTh, ind=idx, v=s.body.th)




    return states


def extract_labels(f):
  """Extract the labels into a 1D uint8 numpy array [index].
  Args:
    f: A file object that can be passed into a gzip reader.
    one_hot: Does one hot encoding for the result.
    num_classes: Number of classes for the one hot encoding.
  Returns:
    labels: a 1D uint8 numpy array.
  Raises:
    ValueError: If the bystream doesn't start with 2049.
  """
    return labels


class DataSet(object):

  def __init__(self, states, labels):
    """Construct a DataSet.
    """
    assert states.shape[0] == labels.shape[0], ('images.shape: %s labels.shape: %s' % (states.shape, labels.shape))
    self._num_examples = states.shape[0]

    ## DO RESCALING

    self._states = states
    self._labels = labels
    self._epochs_completed = 0
    self._index_in_epoch = 0

  @property
  def images(self):
    return self._states

  @property
  def labels(self):
    return self._labels

  @property
  def num_examples(self):
    return self._num_examples

  @property
  def epochs_completed(self):
    return self._epochs_completed

  def next_batch(self, batch_size):
      start = self._index_in_epoch

      if start + batch_size > self._num_examples:
          # Finished epoch
          self._epochs_completed += 1
          # Get the rest examples in this epoch
          rest_num_examples = self._num_examples - start
          states_rest_part = self._states[start:self._num_examples]
          labels_rest_part = self._labels[start:self._num_examples]
          # Start next epoch
          start = 0
          self._index_in_epoch = batch_size - rest_num_examples
          end = self._index_in_epoch
          states_new_part = self._states[start:end]
          labels_new_part = self._labels[start:end]
          return numpy.concatenate((states_rest_part, states_new_part), axis=0), numpy.concatenate(
              (labels_rest_part, labels_new_part), axis=0)
      else:
          self._index_in_epoch += batch_size
          end = self._index_in_epoch
          return self._states[start:end], self._labels[start:end]


def read_data_sets(train_dir, validation_size=5000):

  validation_images = train_images[:validation_size]
  validation_labels = train_labels[:validation_size]
  train_images = train_images[validation_size:]
  train_labels = train_labels[validation_size:]

  train = DataSet(train_images, train_labels, **options)
  validation = DataSet(validation_images, validation_labels, **options)
  test = DataSet(test_images, test_labels, **options)

  return datasets(train=train, validation=validation, test=test)


def load_mnist(train_dir='MNIST-data'):
  return read_data_sets(train_dir)